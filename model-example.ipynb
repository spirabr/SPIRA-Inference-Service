{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPIRA INFERENCE SYSTEM MODEL TEMPLATE\n",
    "\n",
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.pyfunc import PythonModel\n",
    "# only necessary if environment variables were not set yet.\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Class Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "class ModelTemplate(PythonModel):\n",
    "\n",
    "    def load_context(self, context) -> None:\n",
    "      \"\"\"\n",
    "        This method is called as soon as the model is instantiated.\n",
    "\n",
    "        The same context will also be available during calls to predict, \n",
    "        but it may be more efficient to override this method and load \n",
    "        artifacts from the context at model load time.\n",
    "\n",
    "        :param context: A :class:`~PythonModelContext` instance containing artifacts that the model\n",
    "                        can use to perform inference.\n",
    "      \"\"\"\n",
    "      pass\n",
    "\n",
    "    def predict(self, context, model_input) -> Tuple[List[float],str]:\n",
    "      \"\"\"\n",
    "      Evaluates the inference files and metadata and returns a prediction.\n",
    "\n",
    "      :param context: A :class:`~PythonModelContext` instance containing artifacts that the model\n",
    "                      can use to perform inference.\n",
    "      :param model_input: A list of two dictionaries, the first containing inference metadata and\n",
    "                      the second containing audio file bytearrays.\n",
    "      \n",
    "      returns: A Tuple where the first element is a list of numbers and the second is the diagnosis\n",
    "      string (\"positive\", \"negative\", \"inconclusive\"). \n",
    "      \"\"\"\n",
    "      pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I/O Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_credentials():\n",
    "  AWS_ACCESS_KEY_ID = \"access-key-in-prod\"\n",
    "  AWS_SECRET_ACCESS_KEY = \"secret-acess-key-in-prod\"\n",
    "  MLFLOW_S3_ENDPOINT_URL = \"artifact-storage-minio-endpoint\"\n",
    "\n",
    "  os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_ACCESS_KEY_ID\n",
    "  os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET_ACCESS_KEY\n",
    "  os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = MLFLOW_S3_ENDPOINT_URL\n",
    "\n",
    "setup_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_model():\n",
    "  MLFLOW_CONN_URL = \"connection-url-of-mlflow-server\"\n",
    "\n",
    "  # connect to mlflow server\n",
    "  mlflow.set_tracking_uri(MLFLOW_CONN_URL)\n",
    "\n",
    "  # instantiates model\n",
    "  model = ModelTemplate()\n",
    "\n",
    "  # register model\n",
    "  mlflow.pyfunc.log_model(\n",
    "    artifact_path=\"path-in-minio-to-model-artifacts\",\n",
    "    registered_model_name=\"model-name\",\n",
    "    python_model=model\n",
    "  )\n",
    "\n",
    "register_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The model should appear in the MLFlow server, where you will able to see the past versions and also control the staging and production versions of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering the model in the API Database\n",
    "\n",
    "The model now needs to be registered in the system database so that it becomes accessible from the front-end.\n",
    "\n",
    "Send a POST request (it can be thorugh a script or an app like Postman) to the endpoint `/models` of the inference system API, specifying the model `name` and `publishing_channel` in the request body.\n",
    "The name is the name to be displayed in the front-end, while publishing channel is the NATS topic in which the API will send messages so that the Model Server can listen to it and make the predictions. \n",
    "\n",
    "Note: you need user credentials to make the request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the Model\n",
    "\n",
    "Once the model is registered in the database, the only step left is to lift a server with the model. In order to do that, create a new directory in the machine where the API is hosted and clone the GitHub repository SPIRA-Inference-System and fill the environment variables.\n",
    "\n",
    "Particularly, in the file `server_envs/message_listener.env`, the `receiving_channel` should be the `publishing channel` that you registered in the database while `central_channel` should be the channel where the results are sent to the API (this channel is a convention of the deployed system). Be sure that the channels are correct, otherwise the model will not receive the requests.\n",
    "\n",
    "The `model_path` variable specified in `server_envs/model_register.env` is the path of the model you registered in MLFlow. The path follows the convention `models:/<model-name>/<model-version>`. Be sure that you are deploying the right model and the right version of it.\n",
    "\n",
    "Specify the name of the service at `docker-compose.server.yml`, `build-server.sh` and `stop-server.sh` as the name of the model you registered in the database and run `bash build-server.sh`. Please, ensure that the chosen name is not used yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Well Done!\n",
    "Your model is now available for use :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
